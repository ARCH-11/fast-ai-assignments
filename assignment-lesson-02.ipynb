{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 02 - Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a VGG-16 model using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use TensorFlow backend, thus we will explicitly set the Theano image ordering\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters and helper functions\n",
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((3,1,1))\n",
    "\n",
    "def vgg_preprocess(x):\n",
    "    \"\"\"\n",
    "        Subtracts the mean RGB value, and transposes RGB to BGR.\n",
    "        The mean RGB was computed on the image set used to train the VGG model.\n",
    "        Args: \n",
    "            x: Image array (height x width x channels)\n",
    "        Returns:\n",
    "        Image array (height x width x transposed_channels)\n",
    "    \"\"\"\n",
    "    x = x - vgg_mean\n",
    "    return x[:, ::-1] # reverse axis rgb->bgr\n",
    "\n",
    "def conv_block(n_layers, n_filters):\n",
    "    \"\"\"Adds a convolutional block.\"\"\"\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(n_filters, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "def fc_block():\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Lambda(vgg_preprocess, input_shape=(3,224,224), output_shape=(3,224,224)))\n",
    "conv_block(2, 64)\n",
    "conv_block(2, 128)\n",
    "conv_block(3, 256)\n",
    "conv_block(3, 512)\n",
    "conv_block(3, 512)\n",
    "model.add(Flatten())\n",
    "fc_block()\n",
    "fc_block()\n",
    "model.add(Dense(1000, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load weights\n",
    "file_path = 'http://files.fast.ai/models/'\n",
    "file_name = 'vgg16.h5'\n",
    "model.load_weights(get_file(file_name, file_path+file_name, cache_subdir='models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load imagenet class index\n",
    "file_path = 'http://files.fast.ai/models/'\n",
    "file_name = 'imagenet_class_index.json'\n",
    "json_path = get_file(file_name, file_path+file_name, cache_subdir='models')\n",
    "# load json\n",
    "with open(json_path) as f:\n",
    "    class_dict = json.load(f)\n",
    "imagenet_classes = [class_dict[str(i)][1] for i in range(len(class_dict))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['tench', 'goldfish', 'great_white_shark', 'tiger_shark', 'hammerhead']\n"
     ]
    }
   ],
   "source": [
    "print(len(imagenet_classes))\n",
    "print(imagenet_classes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['cats', 'dogs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the last layer and add a layer for cat and dog\n",
    "# We can remove the last added layer in a Sequential model by calling .pop()\n",
    "model.pop() # removed the softmax layer\n",
    "for layer in model.layers: layer.trainable=False\n",
    "# add last layer\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Generators\n",
    "gen = image.ImageDataGenerator()\n",
    "\n",
    "train_batches = gen.flow_from_directory(\n",
    "    'data/train-fast-ai',\n",
    "    target_size=(224,224),\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size)\n",
    "valid_batches = gen.flow_from_directory(\n",
    "    'data/valid-fast-ai',\n",
    "    target_size=(224,224),\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(train_batches.samples)\n",
    "print(valid_batches.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "717/718 [============================>.] - ETA: 1s - loss: 0.4683 - acc: 0.8679Epoch 00000: saving model to saved/epochs/weights-000-val_loss-0.24409.hdf5\n",
      "718/718 [==============================] - 894s - loss: 0.4680 - acc: 0.8678 - val_loss: 0.2441 - val_acc: 0.9178\n",
      "Epoch 2/5\n",
      "717/718 [============================>.] - ETA: 1s - loss: 0.4334 - acc: 0.8872Epoch 00001: saving model to saved/epochs/weights-001-val_loss-0.22265.hdf5\n",
      "718/718 [==============================] - 891s - loss: 0.4343 - acc: 0.8871 - val_loss: 0.2227 - val_acc: 0.9294\n",
      "Epoch 3/5\n",
      "717/718 [============================>.] - ETA: 1s - loss: 0.4365 - acc: 0.8880Epoch 00002: saving model to saved/epochs/weights-002-val_loss-0.26456.hdf5\n",
      "718/718 [==============================] - 876s - loss: 0.4374 - acc: 0.8880 - val_loss: 0.2646 - val_acc: 0.9243\n",
      "Epoch 4/5\n",
      "717/718 [============================>.] - ETA: 1s - loss: 0.4338 - acc: 0.8912Epoch 00003: saving model to saved/epochs/weights-003-val_loss-0.23577.hdf5\n",
      "718/718 [==============================] - 876s - loss: 0.4342 - acc: 0.8911 - val_loss: 0.2358 - val_acc: 0.9339\n",
      "Epoch 5/5\n",
      "717/718 [============================>.] - ETA: 1s - loss: 0.4433 - acc: 0.8905Epoch 00004: saving model to saved/epochs/weights-004-val_loss-0.26432.hdf5\n",
      "718/718 [==============================] - 876s - loss: 0.4430 - acc: 0.8906 - val_loss: 0.2643 - val_acc: 0.9212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b767c0c50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mkdir -p saved/epochs\n",
    "\n",
    "save_name = 'weights-{epoch:03d}-val_loss-{val_loss:.5f}.hdf5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'saved/epochs/'+save_name, monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "    mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Train\n",
    "model.fit_generator(\n",
    "    train_batches, steps_per_epoch=train_batches.samples//batch_size,\n",
    "    epochs=5, validation_data=valid_batches,\n",
    "    validation_steps=valid_batches.samples//batch_size,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save weights\n",
    "%mkdir -p saved\n",
    "model.save_weights('saved/saved_weights_for_full.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "gen = image.ImageDataGenerator()\n",
    "\n",
    "test_batches = gen.flow_from_directory(\n",
    "    './data/test',\n",
    "    target_size=(224,224),\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_batches, test_batches.samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils)\n",
    "from utils import submit2redux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as ./data/subm_full.csv.\n"
     ]
    }
   ],
   "source": [
    "submit2redux(test_batches, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
