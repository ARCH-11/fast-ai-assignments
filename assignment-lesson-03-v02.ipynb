{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 03 - v02 - Under and Over Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a VGG-16 model using Keras. This is the same notebook i used for lesson 02, here i will only remove the dropouts for dense layers and make them trainable too, then we will finetune the network and see if the performance increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use TensorFlow backend, thus we will explicitly set the Theano image ordering\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here i will remove dropouts by setting the p=0. (Dropout(0.)) this will make each node accessible by not removing any of them. In addition we will not need to double the saved dense layer weights, since keras uses inverted dropout which makes the training weights the same as with the case of not using any dropout. https://www.quora.com/What-is-inverted-dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and helper functions\n",
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((3,1,1))\n",
    "\n",
    "def vgg_preprocess(x):\n",
    "    \"\"\"\n",
    "        Subtracts the mean RGB value, and transposes RGB to BGR.\n",
    "        The mean RGB was computed on the image set used to train the VGG model.\n",
    "        Args: \n",
    "            x: Image array (height x width x channels)\n",
    "        Returns:\n",
    "        Image array (height x width x transposed_channels)\n",
    "    \"\"\"\n",
    "    x = x - vgg_mean\n",
    "    return x[:, ::-1] # reverse axis rgb->bgr\n",
    "\n",
    "def conv_block(n_layers, n_filters):\n",
    "    \"\"\"Adds a convolutional block.\"\"\"\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(n_filters, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "# Here i will remove dropouts by setting the p=0. (Dropout(0.))\n",
    "# this will make each node accesable by not removing any of them.\n",
    "# In addition we will not need to double the saved dense layer weights,\n",
    "# since keras uses inverse dropout which makes the training weights\n",
    "# the same as with the case of not using any dropout. \n",
    "def fc_block():\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Lambda(vgg_preprocess, input_shape=(3,224,224), output_shape=(3,224,224)))\n",
    "conv_block(2, 64)\n",
    "conv_block(2, 128)\n",
    "conv_block(3, 256)\n",
    "conv_block(3, 512)\n",
    "conv_block(3, 512)\n",
    "model.add(Flatten())\n",
    "fc_block()\n",
    "fc_block()\n",
    "model.add(Dense(1000, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "file_path = 'http://files.fast.ai/models/'\n",
    "file_name = 'vgg16.h5'\n",
    "model.load_weights(get_file(file_name, file_path+file_name, cache_subdir='models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load imagenet class index\n",
    "file_path = 'http://files.fast.ai/models/'\n",
    "file_name = 'imagenet_class_index.json'\n",
    "json_path = get_file(file_name, file_path+file_name, cache_subdir='models')\n",
    "# load json\n",
    "with open(json_path) as f:\n",
    "    class_dict = json.load(f)\n",
    "imagenet_classes = [class_dict[str(i)][1] for i in range(len(class_dict))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['tench', 'goldfish', 'great_white_shark', 'tiger_shark', 'hammerhead']\n"
     ]
    }
   ],
   "source": [
    "print(len(imagenet_classes))\n",
    "print(imagenet_classes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['cats', 'dogs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the final output layer again but in addition we will leave the dense layers (and flattened layer for sure) trainable. Only the conv layers will be un-trainable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# get the last conv layer index\n",
    "last_conv_index = [i for i,layer in enumerate(model.layers) if type(layer) is Convolution2D][-1]\n",
    "\n",
    "print(last_conv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last layer and add a layer for cat and dog\n",
    "# We can remove the last added layer in a Sequential model by calling .pop()\n",
    "model.pop() # removed the softmax layer\n",
    "for layer in model.layers[:last_conv_index+1]:\n",
    "    layer.trainable=False\n",
    "    #print(type(layer))\n",
    "# add last layer\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use RMSprop and since we are really **fine** tuning the dense layers, we will choose a very small learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=RMSprop(lr=0.00001, rho=0.7), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Generators\n",
    "gen = image.ImageDataGenerator()\n",
    "\n",
    "train_batches = gen.flow_from_directory(\n",
    "    'data/train-fast-ai',\n",
    "    target_size=(224,224),\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "valid_batches = gen.flow_from_directory(\n",
    "    'data/valid-fast-ai',\n",
    "    target_size=(224,224),\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(train_batches.samples)\n",
    "print(valid_batches.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "718/718 [==============================] - 149s - loss: 0.1877 - acc: 0.9359 - val_loss: 0.1184 - val_acc: 0.9592\n",
      "Epoch 2/7\n",
      "718/718 [==============================] - 148s - loss: 0.0356 - acc: 0.9891 - val_loss: 0.1427 - val_acc: 0.9634\n",
      "Epoch 3/7\n",
      "718/718 [==============================] - 147s - loss: 0.0079 - acc: 0.9983 - val_loss: 0.1740 - val_acc: 0.9665\n",
      "Epoch 4/7\n",
      "718/718 [==============================] - 146s - loss: 0.0033 - acc: 0.9997 - val_loss: 0.2140 - val_acc: 0.9685\n",
      "Epoch 5/7\n",
      "718/718 [==============================] - 146s - loss: 0.0025 - acc: 0.9997 - val_loss: 0.2466 - val_acc: 0.9680\n",
      "Epoch 6/7\n",
      "718/718 [==============================] - 145s - loss: 0.0025 - acc: 0.9997 - val_loss: 0.2270 - val_acc: 0.9705\n",
      "Epoch 7/7\n",
      "718/718 [==============================] - 146s - loss: 0.0026 - acc: 0.9998 - val_loss: 0.2232 - val_acc: 0.9695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0c04341160>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model.fit_generator(\n",
    "    train_batches, steps_per_epoch=train_batches.samples//batch_size,\n",
    "    epochs=7, validation_data=valid_batches,\n",
    "    validation_steps=valid_batches.samples//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not bad... We have passed the previous underfitting val accuracy of ~0.92%. And train set is now overfitting, Jeremy uses data agmentation techniques to overcome this situation which i will not go that further now for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "%mkdir -p saved\n",
    "model.save_weights('saved/saved_weights_without_dropout.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "gen = image.ImageDataGenerator()\n",
    "\n",
    "test_batches = gen.flow_from_directory(\n",
    "    './data/test',\n",
    "    target_size=(224,224),\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_batches, test_batches.samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils)\n",
    "from utils import submit2redux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as ./data/subm_without_dropout3.csv.\n"
     ]
    }
   ],
   "source": [
    "submit2redux(test_batches, preds, file_name='subm_without_dropout3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
